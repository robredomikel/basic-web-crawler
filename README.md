# Basic Web Crawler

This publication consists on the crawling pipeline developed during the course 'Statistcal Methods for Text Data Analysis' offered in Tampere University. This chosen functions show the logic of a well implemented web crawler, that can be used for different other crawling projects.

## Table of contents

- [Short description](https://github.com/robredomikel/basic-web-crawler#short-description)
- [What each function does](https://github.com/robredomikel/basic-web-crawler#what-each-function-does)
	- [webCrawler](https://github.com/robredomikel/basic-web-crawler#webcrawler)
	- [gettextlist](https://github.com/robredomikel/basic-web-crawler#gettextlist)
	- [filecrawler](https://github.com/robredomikel/basic-web-crawler#filecrawler)
- [Necessary libraries](https://github.com/robredomikel/basic-web-crawler#necessary-libraries)
- [Short note about preprocessing each text](https://github.com/robredomikel/basic-web-crawler#short-note-about-preprocessing-each-text)
- [References](https://github.com/robredomikel/basic-web-crawler#references)

## Short description

Our task was to create a pipeline that would browse into [Project Gutenberg](https://www.gutenberg.org) webpage and would move to the [Top downloaded 100 EBooks last 7 days](https://www.gutenberg.org/browse/scores/top#books-last30). Then, it would download the amount of wanted ebooks starting from the most downloaded one and move into ascending order into a local directory, fetch all of them one by one and store them into a list, in order to manipulate or work with them.

## What each function does

### webCrawler

### gettextlist

### filecrawler

## Necessary libraries

## Short note about preprocessing each text

## References


{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells requires notebook package.\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "def webCrawler(mywebpage_url, max_ebooks):\n",
    "    # Request to get all the content from the seed webpage.\n",
    "    mywebpage_html=requests.get(mywebpage_url)\n",
    "    \n",
    "    # Parse the HTML content using beautifulsoup\n",
    "    mywebpage_parsed = bs4.BeautifulSoup(mywebpage_html.content,'html.parser')\n",
    "    #print(mywebpage_parsed)\n",
    "    \n",
    "    # During this whole process until the for loop we need to take a look into the developer mode of the \n",
    "    #  given webpage in order to fetch the desired part of the webpage, and fetch it through our following commands.\n",
    "    \n",
    "    # Finds all the \"ol\" type features in the body of the parsed webpage.\n",
    "    webpage_body_elements = mywebpage_parsed.find('body').find_all('ol') \n",
    "    #print(webpage_body_elements)\n",
    "\n",
    "    # Our webpage has 6 types of rankings, from which the 5th one (4 index notation) is our top 100 ebooks last 30 days.\n",
    "    # We fetch that one.\n",
    "    hundred_ebooks_thirty_days = webpage_body_elements[4]\n",
    "    \n",
    "    # Finds all the type \"a\" lines where the information about each books is stored (title, address extension, classification \n",
    "    # NÂº...)\n",
    "    ebooks_links = hundred_ebooks_thirty_days.find_all('a')\n",
    "    \n",
    "    for i in range(0, max_ebooks): # Takes the index values until the maximum size of the length set in the function call.\n",
    "        \n",
    "        # FIRST\n",
    "        each_book_link = ebooks_links[i] # Goes over all links in our list.\n",
    "        book_title = each_book_link.contents[0] # We see that the first content is always the title, we fetch it directly.\n",
    "        book_href = each_book_link['href'] \n",
    "        # \"href\" gives us the extension we need to attach to the parent link in order to find\n",
    "        # our desired location in the webpage.\n",
    "        \n",
    "        # SECOND: Creating the new webpage where the different reading options are located, here we find the link for the txt \n",
    "        # format link.\n",
    "        new_URL = url + book_href \n",
    "        new_webpage_html = requests.get(new_URL) # We newly repeat the process with our new link, we rwquest the content.\n",
    "        newpage_Parsed = bs4.BeautifulSoup(new_webpage_html.content,'html.parser') # Parse in order to remove the html content.\n",
    "        \n",
    "        print(\"Downloading...\")\n",
    "        print('Printing the ebook:', book_title)\n",
    "\n",
    "        webpage_body_elements = newpage_Parsed.find('body') # Find the elements in the body.\n",
    "        # Here we encounter the situation in which there are two types of defined \"a\" type content. We define both in order\n",
    "        # to consider all the possible cases that may appear in each book's txt file link.\n",
    "        text_plain_anchor = webpage_body_elements.find_all('a', type=\"text/plain\")\n",
    "        text_plain_charset_anchor = webpage_body_elements.find_all('a', type=\"text/plain; charset=utf-8\")\n",
    "        \n",
    "        # We define the conditions to choose the type of text anchor.\n",
    "        if(len(text_plain_anchor) == 0): # If the initial one is empty, we exclude it. \n",
    "            print(text_plain_charset_anchor)\n",
    "            text_plain_anchor = text_plain_charset_anchor\n",
    "        else: # ELse, we consider it.\n",
    "            print(text_plain_anchor)\n",
    "          \n",
    "        # THIRD:\n",
    "        link_extension = text_plain_anchor[0]['href'] # Again, we get the link extension.\n",
    "        pathURL = url + link_extension # attach it to the initial parent link.\n",
    "        print(pathURL)\n",
    "        \n",
    "        # FOURTH: Open the web file, set the name and location where the downloaded file will be stored.\n",
    "        with open('eLibrary/' + book_title.replace(\":\", \"\") + '.txt', 'wb') as file:   \n",
    "            response = requests.get(pathURL) # get the content from the link.\n",
    "            file.write(response.content) # Download it's content into our created file.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEB CRAWLER FUNCTION CALL\n",
    "max_ebooks = 20\n",
    "mywebpage_url = 'https://www.gutenberg.org/browse/scores/top#books-last30'\n",
    "\n",
    "webCrawler(mywebpage_url, max_ebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gettextlist FUNCTION DEFINITION #\n",
    "# PARAMS: directory_path (str) - Path of the working directory were the ebooks are downloaded.\n",
    "\n",
    "def gettextlist(directory_path):\n",
    "    directory_textfiles=[] # List of .txt files\n",
    "    directory_nontextfiles=[] # List of non .txt files\n",
    "    directory_nonfiles=[] # List of non file features.\n",
    "    \n",
    "    # Process each item in the directory\n",
    "    directory_contents=os.listdir(directory_path) # Getter of all the content inside the directory\n",
    "    for contentitem in directory_contents:\n",
    "        temp_fullpath=os.path.join(directory_path, contentitem)\n",
    "        \n",
    "        # Non-files (e.g. subdirectories) are stored separately\n",
    "        if os.path.isfile(temp_fullpath)==0:\n",
    "            directory_nonfiles.append(contentitem)\n",
    "        else:\n",
    "            # Is this a non-text file (not ending in .txt)?\n",
    "            if temp_fullpath.find('.txt')==-1:\n",
    "                directory_nontextfiles.append(contentitem)\n",
    "            else:\n",
    "                # This is a text file\n",
    "                directory_textfiles.append(contentitem)\n",
    "    \n",
    "    return(directory_textfiles,directory_nontextfiles,directory_nonfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching all the .txt files from our directory #\n",
    "# PARAMS: directory_path (str) - Brings the directory path of our working directory in our machine.\n",
    "\n",
    "def filecrawler(directory_path):\n",
    "    # Store filenames read and their text content\n",
    "    num_files_read = 0 # Flag variable to keep control of the number of files processed.\n",
    "    crawled_booknames = [] # For titles\n",
    "    crawled_ebooks = [] # For txt files.\n",
    "    \n",
    "    # Gets the text files, non text files and non file features from our directory path.\n",
    "    directory_contentlists = gettextlist(directory_path) \n",
    "    \n",
    "    # In this basic crawled we just process text files and do not handle subdirectories\n",
    "    directory_textfiles = directory_contentlists[0]\n",
    "    \n",
    "    # For loop: goes over the content of all the text files of our selected directory.\n",
    "    for contentitem in directory_textfiles:\n",
    "        #print('Reading file:')\n",
    "        #print(contentitem)\n",
    "        \n",
    "        # Open the file and read its contents (In our case the ebooks)\n",
    "        temp_fullpath = os.path.join(directory_path, contentitem)\n",
    "        temp_file = open(temp_fullpath,'r',encoding='utf-8',errors='ignore')\n",
    "        temp_text = temp_file.read()\n",
    "        temp_file.close()\n",
    "        \n",
    "        # Fetches the header and footer messages and stores their index inside the ebook (Index of ALL the text)\n",
    "        # NOTE: It takes the first index number, not the range it covers.\n",
    "        headerIndex1 = temp_text.find('*** START OF THE PROJECT GUTENBERG EBOOK')\n",
    "        headerIndex2 = temp_text.find('*** START OF THIS PROJECT GUTENBERG EBOOK')\n",
    "        headerIndex3 = temp_text.find('***START OF THE PROJECT GUTENBERG EBOOK')\n",
    "        \n",
    "        if headerIndex1 != -1: # If not equal to 'not existing'(=-1)\n",
    "            actualHeaderIndex = headerIndex1\n",
    "        elif headerIndex2 != -1:\n",
    "            actualHeaderIndex = headerIndex2\n",
    "        else:\n",
    "            actualHeaderIndex = headerIndex3\n",
    "        \n",
    "        # We consider the next line after the header index we identified in our set of conditions.\n",
    "        actualHeaderIndex = temp_text.find(\"\\n\", actualHeaderIndex)\n",
    "        \n",
    "        # Same logic for the footer.\n",
    "        actualFooterIndex1 = temp_text.find('*** END OF THE PROJECT GUTENBERG EBOOK')\n",
    "        actualFooterIndex2 = temp_text.find('*** END OF THIS PROJECT GUTENBERG EBOOK')\n",
    "        actualFooterIndex3 = temp_text.find('***END OF THE PROJECT GUTENBERG EBOOK')\n",
    "        \n",
    "        if actualFooterIndex1 != -1:\n",
    "            actualFooterIndex = actualFooterIndex1\n",
    "        elif actualFooterIndex2 != -1:\n",
    "            actualFooterIndex = actualFooterIndex2\n",
    "        else:\n",
    "            actualFooterIndex = actualFooterIndex3\n",
    "        \n",
    "        print(\"Header Index: \",  actualHeaderIndex, \"Footer index: \", actualFooterIndex)\n",
    "        \n",
    "        # Store the read filename, that is, the name of the book.\n",
    "        crawled_booknames.append(contentitem)\n",
    "        # Only takes the part of the book between the header and footer indexes.\n",
    "        crawled_ebooks.append(temp_text[actualHeaderIndex : actualFooterIndex]) \n",
    "        num_files_read = num_files_read+1\n",
    "        \n",
    "    return(crawled_booknames,crawled_ebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycrawled_filenames_and_texts = filecrawler(mydirectory_path)\n",
    "mycrawled_filenames = mycrawled_filenames_and_texts[0] # Names all the .txt files\n",
    "mycrawled_texts = mycrawled_filenames_and_texts[1] # Contents of all the text files, that is, the ebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
